{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Overview \n",
    " \n",
    " We visit principal component analysis in this chapter as an example of dimension reduction methods. The idea of capture the major sources of variances, i.e., principal components, is a general approach to many statistical procedures. Note that we discuss dimension reduction under the unsupervised learning scenario, meaning that there are no responses in consideration. The principal components characterize the variability in the given data set with relatively small number of variables. When an outcome (response) is specified and we want to find small number of variables to maximize the explanability of the outcome, the corresponding method is known as [sufficient dimension reduction](https://en.wikipedia.org/wiki/Sufficient_dimension_reduction). \n",
    " \n",
    "Recall in Chapter 7, we discuss clustering methods that put observations into distinct clusters. If we think of the $K$ cluster centers (median, mean, etc.) as the representatives of the $n$ observations, we reduce the sample size $n$ to a small amount $K$. If instead, we run clustering methods on the **features** instead of observations. We will find $K$ \"features\" as representative features. When $K < p$, we have reduced the dimenion of the data set. \n",
    "\n",
    "Dimension reduction methods find a low-dimensional representation of the observations that contain a good fraction of information in the original data. The primary task for dimension reduction is to reduce the dimension, whereas the cluster analysis aims to find homogeneous subgroups. It is possible to use cluster analyses for dimension reduction, for instance, treat the cluster centers as representative features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical  methods of dimension reduction includes \n",
    "- Principal Component Analysis (PCA)\n",
    "- Multi-Dimensional Scaling (MDS)\n",
    "- Kernel PCA, Sparse PCA, Manifold Learning, ...\n",
    "\n",
    "These can generally be thought of as soft clustering or as vector space embedding. We only discuss the principal component analysis in this chapter. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
