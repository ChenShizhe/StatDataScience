{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Overview of Statistical Data Science "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 What is data science?\n",
    "\n",
    "Data science is an interdisciplinary field. \n",
    "- It uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data.\n",
    "- It involves techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, domain knowledge and information science.\n",
    "- It is related to data mining, machine learning and big data. \n",
    "\n",
    "\n",
    "**Remark** (Data science and statistics) People have very different views regarding the two terms. \n",
    "- Everyone agrees that statistics is a crucial component of data science\n",
    "- Some argued that data science is not a new field, but rather another name for statistics.\n",
    "- Some see that data science is applied statistics.\n",
    "Compared with _traditional_ statistics (e.g., in the 70s), data science \n",
    "- deals with new types of data (e.g., images, electronic health record),\n",
    "- deals with huge datasets,\n",
    "- and emphasizes prediction and action.\n",
    "\n",
    "\n",
    "\n",
    "A crude schematic of a common data science project is show below. \n",
    "<img src=\"../Figures/Ch1/datasci1.jpg\" alt=\"lm\" style=\"width: 500px;\"/>\n",
    "We can roughly categorize tasks of a data scientist based on the schematic. \n",
    "\n",
    "| Task | Descriptions | Skills required |\n",
    "| :-: | :- | :- |\n",
    "| Visioning | To generate hypotheses or questions that are of interest  | Domain knowledge, self-learning, quantitative methods, etc. |\n",
    "| Data acquisition | To gather data for verifying hypotheses via experiments, sample surveys, or data queries | Experimental designs, causal inference, query languages, etc. |\n",
    "| Analysis | To analyze data to produce meaningful visualizations, build predictive models, test hypotheses, etc. | Domain knowledge, programming languages, statistical models, statistical theory, optimization, etc.  |\n",
    "| Conclusion | To conclude the analysis results in writing or via presentations | Communication skills, writing skills, public speaking skills, etc.  |\n",
    "| Management | To manage and oversee the project | Leadership, collaboration skills, tools for team projects, etc. |\n",
    "\n",
    "\n",
    "This collection of notes focuses on the __Data__ part of the chain. \n",
    "<img src=\"../Figures/Ch1/datasci2.jpg\" alt=\"lm\" style=\"width: 500px;\"/>\n",
    "The transform-visualise-model step is in fact an iterative process, where we need to reflect and revise our approaches based on results in previous steps. \n",
    "<img src=\"../Figures/Ch1/datasci3.jpg\" alt=\"ds\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Example: `wages`\n",
    "\n",
    "We now turn to the `wages` data to see an example of data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we propose a linear model for the response (log income) and the single predictor (education). \n",
    "\n",
    "<img src=\"../Figures/Ch1/lm.jpg\" alt=\"lm\" style=\"width: 400px;\"/>\n",
    "\n",
    "The formula for `lm()` only needs to include the response (variable on the y axis) and predictors (variable on the x-axis). The intercept term is included by default, unless specified otherwise (`-1`). \n",
    "\n",
    "\n",
    "<img src=\"../Figures/Ch1/formula.jpg\" alt=\"lm\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "as.numeric(round(mod_e$coef[1],digits=3))": "8.558",
     "as.numeric(round(mod_e$coef[2],digits=3))": "0.142"
    }
   },
   "source": [
    "We can try to interpret the fitted coefficients.   \n",
    "- The average log income is {{as.numeric(round(mod_e$coef[1],digits=3))}} for those with zero years of education.\n",
    "\n",
    "- The average difference in log income is {{as.numeric(round(mod_e$coef[2],digits=3))}} for groups with one year difference in education. \n",
    "\n",
    "Neither statement makes a lot of sense. For instance, (1) there isn't anyone with zero years of education in the `wages` dataset, and (2) differences in log income are not informative for general audience. Therefore, we have two questions to think about. \n",
    "- Why take the logarithm of income?\n",
    "- How should we interpret the fitted results?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After model fitting, we usually proceed to hypothesis testing, predictive modeling, or model diagnostics. An experienced reader can certainly perform these tasks from scratch. However, it is best not to reinvent the wheels. For very common tasks, it is extremely likely that there are existing tools out there on the Internet. Here we use the package `broom` in `R`. \n",
    "\n",
    "`Broom` includes three functions which work for most types of models (and can be extended to more):  \n",
    "1. `tidy()` - returns model coefficients, stats: what uncertainty is associated with it?  \n",
    "2. `glance()` - returns model diagnostics: how \"good\" is the model?  \n",
    "3. `augment()` - returns predictions, residuals, and other raw values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low $R^2$ indicates that education explains only a part of variability of income. We can include more predictors in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ does improves a bit, but still remains low. Maybe the linear model is a not a good choice. It might be a good idea to look at the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
