{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Prediction\n",
    "\n",
    "\n",
    "This note does **not** intend to give a detailed discussion of prediction methods. Rather, this chapter offers a way of approaching a prediction problem for beginners. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.1 General Prediction Framework\n",
    "\n",
    "Consider a function $f$ indexed by a vector of parameters $\\boldsymbol{\\beta} \\equiv \\left(\\beta_1,\\ldots, \\beta_p\\right)$...\\\\\n",
    "\n",
    "In particular for a vector of observations ${\\bf x} \\equiv \\left(x_1,\\ldots,x_k\\right)$ we can get a prediction by\n",
    "\\[\n",
    "{\\rm prediction}\\   \\gets f\\left({\\bf x},\\boldsymbol{\\beta}\\right)\n",
    "\\]\n",
    "If we change $\\boldsymbol{\\beta}$ our predictions will change.\n",
    "\n",
    "We want to find the best $\\boldsymbol{\\beta}$ for predicting our data.\n",
    "\n",
    "To do this, we search for the vector $\\boldsymbol{\\beta}$ that minimizes\n",
    "\\[\n",
    "\\operatorname{avg}\\left[L\\left( {\\rm outcome} ,  f\\left({\\bf x},\\boldsymbol{\\beta}\\right) \\right)\\right]\n",
    "\\]\n",
    "Often use some heuristic to get a **minimizer**\n",
    "\n",
    "Key components:\n",
    "\n",
    " criteria \n",
    " flexible modeling \n",
    " model selection\n",
    "     criteria\n",
    "     over fitting \n",
    "     test train\n",
    "     cross-validation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Criteria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Continuous outcomes\n",
    "\n",
    "With continuous outcome, often use\n",
    "\\[\n",
    "L\\left(  {\\rm outcome},  {\\rm prediction}\\right) = \\left( {\\rm outcome} - {\\rm prediction}\\right)^2\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does some combination of the features predict outcome effectively?\n",
    "\n",
    "Combining features to predict outcome  \n",
    "\n",
    "Suppose each observation is of the form\n",
    "\\[\n",
    "\\left( {\\rm outcome},  {x_1, \\ldots, x_k}\\right)\n",
    "\\]\n",
    "where $ {x_1, \\ldots, x_k}$ is a set of $k$ numeric features.\n",
    "\n",
    "\n",
    "Note $\\{0,1\\}$ indicators of group membership count as quantitative. \n",
    "\n",
    "What is a simple way to combine features to predict outcome?\n",
    "\n",
    "<span style=\"color:blue\">Multiple linear regression!</span>\n",
    "\n",
    "Choose $\\beta_0,\\beta_1,\\ldots, \\beta_k$ to minimize\n",
    "\n",
    "\\[\n",
    "\\operatorname{avg}L\\left(  {\\rm outcome}, {\\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k}\\right)\n",
    "\\]\n",
    "where $L\\left( {\\rm outcome},  {\\rm prediction}\\right)$ is some **cost** of making <span style=\"color:blue\"> prediction </span>, when the truth is outcome.\n",
    "\n",
    "\n",
    "\n",
    "Often we use \n",
    "\t\\[\n",
    "\tL\\left( {\\rm outcome} ,  {\\rm prediction}  \\right) = \\left( {\\rm outcome}  - {\\rm prediction}\\right)^2\n",
    "\t\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear Regression\n",
    "\tIs the truth often\n",
    "\t\\[\n",
    "\ty = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k + \\epsilon\n",
    "\t\\]\n",
    "    \n",
    "When do I care? \n",
    "If I'm trying to make a statement like... After adjusting for $x_2,\\ldots,x_k$, I find that $x_1$ is associated with $y$\n",
    "It can also result in a not-very-predictive model!\n",
    "\n",
    "The Inference Question \n",
    "\n",
    "We will discuss this later, because it is important. It's a bit of a pipe-dream to believe we can <span style=\"color:red\"> completely</span> adjust for confounding without randomization. Often important to decrease the effect of confounding though. However, it is dangerous to assume that effects are **linear**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##-----------------------------------------------##\n",
    "## Lasso: linear regression with CV ####\n",
    "## \n",
    "\n",
    "cv.linear.fit <- cv.glmnet(as.matrix(covariates.expanded), outcome, family = \"gaussian\")\n",
    "plot(cv.linear.fit)\n",
    "\n",
    "## Report number of features included in the optimal model\n",
    "sum(coef(cv.linear.fit, s = \"lambda.min\") != 0) \n",
    "## Find names of the selected features\n",
    "\n",
    "coefs <- coef(cv.linear.fit, s = \"lambda.min\") \n",
    "linear.selected.variable<-which(coefs[,1]!=0) \n",
    "linear.selected.variable<-linear.selected.variable[linear.selected.variable!=1]-1;\n",
    "linear.selected.mat =as.matrix(covariates.expanded[,linear.selected.variable]);\n",
    "\n",
    "## Refit the model on the full data set with the selected variable:\n",
    "linear.fit<-glmnet(linear.selected.mat, \n",
    "                   outcome, family = \"gaussian\",lambda=0)\n",
    "plot(x=predict(linear.fit,newx=linear.selected.mat),y=outcome)\n",
    "##-----------------------------------------------##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Binary and categorical outcomes \n",
    "\n",
    "What about binary outcomes and categorical outcomes?\n",
    "\n",
    "\n",
    "You may have learned it from previous stat classes:\n",
    "\n",
    "The <span style=\"color:red\"> likelihood </span> of observing a collection of data, given a model \n",
    "\n",
    "We generally choose parameters in a model to maximize that likelihood, which is equivalent to maximizing the \"log-likelihood\" or  minimizing the \"negative-log-likelihood\"\n",
    "\n",
    "In problems with a likelihood, generally our loss is the negative-log-likelihood.\n",
    "\n",
    "\n",
    "\n",
    "For a binary outcome $y$, we will often model the probability $y = 1$ as $f(x)$...\n",
    "\n",
    "Leading us to the negative-log-likelihood\n",
    "\\[\n",
    "L( y,  f(x)) = - y \\cdot \\log\\left(\\frac{ f(x) }{1-f(x) }\\right) + \\operatorname{log}\\left(1- f(x) \\right)\n",
    "\\]\n",
    "Our ML methods will try to choose $f$ to minimize the average of that loss over all observations.\\\\\n",
    "\n",
    "Slightly more traditional to model $g(x) = \\operatorname{log}\\left(\\frac{f(x)}{1-f(x)}\\right)$ (<span style=\"color:red\"> Why? </span>)\n",
    "\n",
    "If we try to minimize the negative-log-likelihood,  and we model \n",
    "\\[\n",
    "\\operatorname{log}\\left(\\frac{f(x)}{1-f(x)}\\right) = x_1\\beta_1 + \\ldots + x_p\\beta_p\n",
    "\\]\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\[\n",
    "    f(x) = \\frac{e^{x_1\\beta_1 + \\ldots + x_p\\beta_p}}{1+ e^{x_1\\beta_1 + \\ldots + x_p\\beta_p}}\n",
    "\\]\n",
    "\n",
    "This is known as the <span style=\"color:blue\"> logistic</span> regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##-----------------------------------------------##\n",
    "## Lasso: logistic regression with CV####\n",
    "##\n",
    "cv.logistic.fit <- cv.glmnet(as.matrix(covariates.expanded), outcome, family = \"binomial\")\n",
    "plot(cv.logistic.fit)\n",
    "\n",
    "## Report number of features included in the optimal model\n",
    "sum(coef(cv.logistic.fit, s = \"lambda.min\") != 0) \n",
    "\n",
    "## Find names of the selected features\n",
    "coefs <- coef(cv.logistic.fit, s = \"lambda.min\") \n",
    "\n",
    "logistic.selected.variable<-which(coefs[,1]!=0) logistic.selected.variable<-logistic.selected.variable[logistic.selected.variable!=1]-1; \n",
    "\n",
    "## Refit the model on the full data set with the selected variable:\n",
    "\n",
    "logistic.selected.mat =as.matrix(covariates.expanded[,logistic.selected.variable]);\n",
    "logistic.fit<-glmnet(logistic.selected.mat, \n",
    "                     outcome, family = \"binomial\",lambda=0)\n",
    "plot(x=predict(logistic.fit,newx=logistic.selected.mat),y=outcome)\n",
    "##-----------------------------------------------##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6.2.3 Categorical Outcome\n",
    "\n",
    "If we observe categorical $y\\in\\{1,\\ldots, K\\}$...\n",
    "We consider $K$ functions $f_1,\\ldots, f_k$  and set the prob $y = j$ as $\\frac{f_j(x)}{\\sum_{k=1}^K f_k(x)}$\n",
    "\n",
    "\n",
    "Leading us to the negative-log-likelihood\n",
    "\\[\n",
    "L( y,  f_1(x),\\ldots, f_K(x)) = -\\log\\left( f_{ y } (x)\\right) + \\operatorname{log}\\left(\\sum_{k=1}^K  f_k(x)\\right)\n",
    "\\]\n",
    "\n",
    "Our ML methods will try to choose $f_1,\\ldots, f_K$ to minimize the average of that loss over all observations.\\\\\n",
    "\n",
    "Slightly more traditional to model $g(x) = \\log(f(x))$.\n",
    "\n",
    "Loss-based Estimation with bells+whistles\n",
    "\n",
    "All of these losses can be combined with\n",
    "- Penalties\n",
    "- Feature Expansions/Dictionaries\n",
    "\n",
    "There we use linear combinations of dictionary elements (e.g., meta-features) to minimize a penalized loss.\n",
    "\n",
    "\n",
    " Big Picture\n",
    "There are lots of candidate methods for building predictive models\n",
    "\n",
    "There is no best method **overall**, but there often is a best method **for your data**. There are two keys for a method to do well on your data:\n",
    "    \n",
    "- It does not <span style=\"color:orange\"> completely</span> miss important structure\n",
    "- It admits a proper amount of <span style=\"color:blue\"> complexity</span> for the <span style=\"color:blue\"> number of observations</span> in your data.\n",
    "\n",
    "\n",
    "\n",
    "The \"proper\" amount of complexity\n",
    "\n",
    "How can we tell if we are choosing a model that is...\n",
    "- <span style=\"color:orange\"> sufficiently complex </span> (to capture the signal)...\n",
    "- but not <span style=\"color:red\"> overly complex!</span> (such that we overfit our training data)?\\\\\n",
    "\n",
    "The proper degree of complexity is **extremely** data-dependent\n",
    "So <span style=\"color:blue\"> data-resampling, split-sample-validation or CV</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.3 Flexible models \n",
    "\t\n",
    "Returning to the **prediction** problem...\n",
    "\n",
    "Sometimes we just want a black-box that makes good predictions. Other times we want to see how much info about response is contained in **any** combination of our assays. \n",
    "\n",
    "We can create a nonlinear model with even only one single feature $x$. \n",
    "- <span style=\"color:orange\">Local smoothing:</span> For each $x$-value, estimate outcome with an average of observations with nearby $x$-values.\n",
    "- <span style=\"color:orange\"> Meta-feature construction:</span> Rather than just using $x$ as our one feature, we use e.g. $x, x^2, \\ldots, x^5$ as features; and then choose $\\beta_0,\\ldots,\\beta_5$ to minimize\n",
    "\t\\[\n",
    "\t\\operatorname{avg}\\left(y - \\left[\\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_5 x^5\\right]\\right)^2\n",
    "\t\\]\n",
    "This is like doing span style=\"color:blue\">multiple regression} on $5$ features...\n",
    "\n",
    "But our additional features are user-defined.\n",
    "\n",
    "Meta-feature expansions (in one dimension)\n",
    "\n",
    "For continuous features various expansions may be used:\n",
    "- Polynomial basis\n",
    "\n",
    "\\[\n",
    "\t\t\\begin{pmatrix} \\,\\\\x\\\\ \\, \\end{pmatrix}\n",
    "\t\t\\rightarrow\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\\, & \\, & \\, & \\, \\\\ x, & x^2, & \\cdots, &  x^m\\\\ \\, & \\, & \\, & \\,\n",
    "\t\t\\end{pmatrix}\n",
    "\\]\n",
    "        \n",
    "- Spline basis\n",
    "\n",
    "\\[\n",
    "\t\t\\begin{pmatrix} \\,\\\\x\\\\ \\, \\end{pmatrix}\n",
    "\t\t\\rightarrow\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t\\, & \\, & \\, & \\, \\\\  x, &  \\left(x - t_1\\right)_{+}, & \\cdots, & \\left(x - t_m\\right)_{+}\\\\ \\, & \\, & \\, & \\,\n",
    "\t\t\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "- Others include higher order spline, Fourier, wavelet, among others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Meta Feature Expansion Example\n",
    "\tWith enough data, and a rich enough basis, many functions can be estimated\n",
    "\t\n",
    "<figure>\n",
    "  <img src=\"../Figures/Ch6/sin.png\" alt=\"sin()\" style=\"width:70%\">\n",
    "  <figcaption> </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Same ideas can be employed with multiple features:\n",
    "\n",
    "For <span style=\"color:orange\">local smoothing</span> we now need a **distance** for multivariate ${\\bf x}$ \n",
    "\t\n",
    "Often use Euclidean distance:\n",
    "\t\\[\n",
    "\tD\\big[\\underbrace{\\left( x_{1},x_{2} \\right)}_{\\textrm{observation $1$}}, \\underbrace{\\left( x_{1},x_{2} \\right)}_{\\textrm{observation $2$}}\\big] \\equiv \\sqrt{\\left( x_{1} - x_{1} \\right)^2 + \\left( x_{2}  -  x_{2} \\right)^2}\n",
    "\t\\]\n",
    "\t\n",
    "Could include weights  (important if we haven't scaled our features) or an appropriate  kernel for interesting data-types.\n",
    "\n",
    "\n",
    "In the  meta-feature approach, many ways to build meta-features: \n",
    "- Additive: e.g. $( x_1 , x_2 ) \\rightarrow ( x_1,x_1^2,x_1^3, x_2,x_2^2,x_2^3)$\n",
    "- Interacting: e.g. $( x_1 ,x_2 ) \\rightarrow ( x_1,x_1^2 ,  x_2,x_2^2 , x_1 x_2 ,  x_1^2 x_2  , x_1 x_2^2 )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##-----------------------------------------------##\n",
    "## Notes ####\n",
    "### We analyzed the NOAH data set and built a prediction model in Homework #2. \n",
    "### What was your conclusion?\n",
    "\n",
    "### Now let's try to build a prediction model on the iris data set  \n",
    "### We will examine alternatives to the two major decisions we can make:\n",
    "###   a) covariates: including them in their original form, or maybe transfer them?\n",
    "###   b) model: fit a logistic regression, or maybe just a linear regression? \n",
    "### Let's try these out!\n",
    "### Remember that we may not be able to build a good prediction model...\n",
    "###  ... if there is really no information in the data set after all! \n",
    "##-----------------------------------------------##\n",
    "\n",
    "\n",
    "##-----------------------------------------------##\n",
    "## Instruction ####\n",
    "### If you are comfortable with R, \n",
    "###     try to write your own code (you can use mine for refence)\n",
    "### If you are struggling with R, \n",
    "###     move my code (at the end of the script) to the right place\n",
    "###     and execute them. \n",
    "##-----------------------------------------------##\n",
    "\n",
    "\n",
    "\n",
    "##-----------------------------------------------##\n",
    "## Read data ####\n",
    "data(iris)\n",
    "plot(iris)\n",
    "names(iris)\n",
    "### Let's try to predict/classify the species using \n",
    "### Using Sepal.Length, Sepal.Width, Petal.Length, Petal.Width \n",
    "### Yes, just 4 variables instead of the 56k variables in NOAH\n",
    "### We will only consider TWO species: setosa and versicolor here\n",
    "keep.ind = which(iris$Species != 'virginica');\n",
    "outcome = iris$Species[keep.ind]=='setosa';\n",
    "covariates=iris[keep.ind,1:4];\n",
    "##-----------------------------------------------##\n",
    "\n",
    "##-----------------------------------------------##\n",
    "## Meta-feature generation ####\n",
    "### Task: construct meta-features based on the 4  variables in covariates \n",
    "### Several options are: \n",
    "### (Orthogonal) Polynomial bases (?poly), splines (?spline),Fourier bases (?create.fourier.basis) (NOTE: this requires the \"fda\" package), among others \n",
    "### The sample script will use the polynomial bases\n",
    "### You should try out a different type of bases if you are fluent with R \n",
    "\n",
    "### Create polynomial bases for one variable and visualize them  \n",
    "M=5; \n",
    "tmp <- covariates[,1];\n",
    "tmp.expanded<-poly(tmp, degree = M,raw=F); # Set raw=T to see the raw polynomials \n",
    "\n",
    "\n",
    "### For visualization, you can do it manually, or use the melt() function in reshape2 \n",
    "#library(reshape2)\n",
    "library(reshape2)\n",
    "tmp.plot <- data.frame(tmp.expanded);\n",
    "names(tmp.plot)= paste('x', 1:M, sep='');\n",
    "tmp.plot.long <- melt(tmp.plot, id=\"x1\")  \n",
    "ggplot(data=tmp.plot.long,\n",
    "       aes(x=x1, y=value, colour=variable)) +\n",
    "  geom_line()\n",
    "\n",
    "### Now create a data frame that contains the expanded features:\n",
    "### Hint: just recycle the code in visualization! (not very elegant but working)\n",
    "for(i in 1:dim(covariates)[2]){\n",
    "  tmp <- covariates[,i];\n",
    "  tmp.name=names(covariates)[i];\n",
    "  tmp.expanded<-poly(tmp, degree = M,raw=F);\n",
    "  tmp.plot <- data.frame(tmp.expanded);\n",
    "  names(tmp.plot)= paste(tmp.name,'Poly',1:M, sep='');\n",
    "  if(i==1){\n",
    "    covariates.expanded= tmp.plot;\n",
    "  }else{\n",
    "    covariates.expanded= cbind(covariates.expanded,tmp.plot);\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Choosing an approach\n",
    "\n",
    "With $3+$ features often better to use meta-feature approach, than local-averaging...\n",
    "\n",
    "Any guesses why local-averaging has trouble?\n",
    "\n",
    "With more features, local neighborhood becomes  sparse\n",
    "\n",
    "Generally, more  uniformative features than informative... So, distance becomes dominated by uninformative component. \n",
    "\n",
    "\n",
    "\n",
    "Sometimes have many candidate features:\n",
    "- Biomolecular 'omics data: $1,000-10^9$ features (eg. genes).\n",
    "- Unstructured text data: $1,000+$ meta-features (eg. word-counts).\n",
    "- Image data: $10,000+$ features (eg. voxels)\n",
    "\n",
    "In many of these scenarios we\n",
    "- Have more features than observations\n",
    "- Believe most features are not informative for response\n",
    "- But do not know which those are!\n",
    "\n",
    "\n",
    "Number of obs determines complexity that can be supported:\n",
    "\n",
    "$100$ obs cannot support even linear model with $1000$ features\n",
    "\n",
    "\n",
    "Too much flexibility $\\longrightarrow$ over-adapts to training data, e.g.\t\n",
    "\n",
    "Univariate smoothing with tiny bandwidth $\\longrightarrow$ interpolates points\n",
    "Restricting the number of features allowed in the model is one way of limiting its complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Tree-Based Methods\n",
    "\n",
    "The main steps of tree-based methods are as follows:\n",
    "\n",
    "- *stratify* (or segment) the predictor space into small and simple regions\n",
    "- for each observation, determine which segment it belongs to; the stratification of space can be captured by a tree (hence the name)\n",
    "- predict the outcome by the mean (regression), mode (classification), or hazard (survival) of outcomes in that segment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Decision Trees: the general idea\n",
    "\n",
    "- The *regression* tree in the above example is likely an over-simplification of the true relationship \n",
    "- However, it is very easy to interpret, and has a nice graphical representation: you can easily explain it to a non-statistician, and do not need a computer (or even a calculator) to get an estimate!\n",
    "\n",
    "The general steps for building a regression (or classification) tree is quite simple:\n",
    "- Divide the predictor space, i.e. the set of possible values for $X_1, X_2, \\ldots, X_p$, into {\\color{red} $J$ distinct and non-overlapping regions}, $R_1, R_2, \\ldots, R_J$.\n",
    "- Use the mean (regression) or mode (classification) of the response values for the training observations in region $R_j$ as the predicted response for that region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "The main question: <span style=\"color:red\"> how should we construct the regions</span> $R_1, \\ldots, R_J$, and <span style=\"color:red\"> how many regions should we use</span>? \n",
    "\n",
    "\n",
    "Partitioning the Predictor Space\n",
    "Suppose $J$ is known. How to construct the regions $R_1, \\ldots, R_J$?\n",
    "- Divide the predictor space into high-dimensional <span style=\"color:red\"> rectangles</span> or boxes (in theory any shape can be used, but this is simpler)\n",
    "- Find boxes $R_1, \\ldots, R_J$ that minimize the RSS\n",
    "\\[\n",
    "\\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - {\\hat{y}_{R_j}})^2\n",
    "\\]\n",
    "- Unfortunately, it is not possible to consider every possible partition of the space into $J$ boxes\n",
    "\n",
    "We consider instead a {\\it greedy} approach called {<span style=\"color:blue\"> recursive binary splitting</span>\n",
    "- top-down: similar to hierarchical clustering\n",
    "- best split is determined at each given step, instead of the best global step (which is computationally difficult)\n",
    "\n",
    "                                                    \n",
    "                                                    \n",
    "- Select a predictor $X_j$ and a <span style=\"color:red\"> cut point</span> $t$ such that splitting the predictor space into regions\n",
    "\n",
    "\\[\n",
    " R_1(j, t) = \\{ X \\mid X_j < t\\}, \\hspace{1cm} R_2(j, t) = \\{ X \\mid X_j \\ge t\\}\n",
    "\\]\n",
    " gives the largest decrease in RSS.\n",
    " - We consider *all predictors* and *all cut points* for each predictor!\n",
    " - We find the value of $j$ and $t$ that minimizes\n",
    "\\[\n",
    " \\sum_{i: x_i \\in R_1(j,t)} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j,t)} (y_i - \\hat{y}_{R_2})^2\n",
    "\\]\n",
    "\n",
    "Finding *optimal* $j$ and $s$ can actually be done very quickly (as long as $p$ is not too large)\n",
    "\n",
    "Repeat the above process, splitting a subspace (rather than the whole space) in each step\n",
    "- Previously used variables can be considered again\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tree Pruning\n",
    "<span style=\"color:red\"> complexity</span> of the regression tree is determined by the number of regions $J$\n",
    "\n",
    "- A tree with large $J$ may work well in training data, but would be very bad on test data\n",
    "- A smaller tree with fewer splits might have lower variance and better interpretation at the cost of a little bias\n",
    "- One option is to consider a split only if the drop in RSS is larger than some (high) threshold\n",
    "- However, this may not be a good strategy since a so-so split at step $j$ may be followed by a great one at step $j+1$\n",
    "- Instead, we <span style=\"color:blue\"> first grow a large tree</span>, e.g. until no region has $>5$ observations, and then <span style=\"color:red\"> prune</span> it to obtain a <span style=\"color:red\"> subtree</span>\n",
    "- And, again, we (basically) use <span style=\"color:red\"> cross validation</span> to select $J$\n",
    "\n",
    "\n",
    "Our goal is to select a subtree that leads to the {\\color{red} lowest test error}\n",
    "\n",
    "We can use CV, however, it will be too computationally expensive to estimate the CV error for every possible subtree!\n",
    "- Instead we use a strategy called <span style=\"color:red\"> cost complexity pruning</span> a.k.a. <span style=\"color:red\"> weakest link pruning</span>\n",
    "- Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter $\\alpha$\n",
    "- For each value of $\\alpha$, there exists a subtree $T \\subset T_0$ such that \n",
    "\\[\n",
    "\\sum_{k=1}^{|T|}\\sum_{x_i \\in R_k} (y_i - \\hat{y}_{R_k})^2 + \\alpha |T|\n",
    "\\]\n",
    " is as small as possible ($|T|$ is the number of leaves of the tree) \n",
    "- $\\alpha$ controls the tradeoff between complexity and fit (sound familiar?)\n",
    "- We can select $\\alpha$ using validation set or CV approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Extensions of Trees \n",
    "Trees can be seen as a loss-based ML method\n",
    "\n",
    "As such, trees can extend naturally to binary/survival data choose splits greedily to minimize loss\n",
    "                                                    \n",
    "                                                    \n",
    "Trees are often used as <span style=\"color:blue\"> base learners</span>\n",
    "<span style=\"color:red\"> model aggregation/ensembling</span>, e.g.\n",
    "- Boosting\n",
    "- Bagging\n",
    "- Stacking\n",
    "\n",
    "Trees in high dimensions\n",
    "Some `R` functions are better equipped to deal with high dimensional data than others...\n",
    "Rarely will it hurt your method to prescreen out features that are extremely marginally uncorrelated with outcome.\n",
    "Remember to be careful how you do this, with validation!\n",
    "\n",
    "Another approach is to screen out features with very low variance\n",
    "Alternatively, some packages handle big data better (eg. `Ranger` / `XGBoost`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##---------------------------------------##\n",
    "## Decistion tree ####\n",
    "### \n",
    "library(tree)\n",
    "\n",
    "fit_tree <- tree(Y ~ Xmat)\n",
    "\n",
    "cv_fit_tree <- cv.tree(fit_tree)\n",
    "\n",
    "plot(cv_fit_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "\n",
    "### Selecting an appropriate model\n",
    "\n",
    "Model selection has many meanings \n",
    "- It can mean selecting a $\\lambda$-value in the lasso, known as <span style=\"color:blue\"> tuning parameter selection</span>\n",
    "- It can mean selecting covariates and how they are to be included in the model, known as feature selection, and sometimes feature generation \n",
    "- It can mean choosing between a regression tree (random forest), neural network (deep learning), or logistic regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we use feature selection to fit a model with only a small number of those features.\n",
    "\n",
    "One method is **best subset selection**:\n",
    "\n",
    "Choose $\\beta_0,\\beta_1,\\ldots, \\beta_k$, with no more than $C$ of those $k$ nonzero to minimize\n",
    "\\[\n",
    "\t\\operatorname{avg}L\\left({\\rm outcome} , {\\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k}\\right)\n",
    "\\]\n",
    "\t   \n",
    "Here we use a constraint on the allowable $\\beta_0,\\ldots,\\beta_k$ to select our features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In practice *best subset selection* is computationally intractable. Use a slightly different approach, called the **Lasso**\n",
    "\n",
    "We choose $\\beta_0,\\beta_1,\\ldots, \\beta_k$, to minimize\n",
    "\t\\[\n",
    "\t\\operatorname{avg}\\left[L\\left( {\\rm outcome} ,  {\\beta_0 + \\beta_1 x_1 + ... +\\beta_k x_k}\\right)\\right] + \\lambda P\\left(\\beta_1,\\ldots,\\beta_k\\right)\n",
    "\t\\]\n",
    "\twith $ P\\left(\\beta_1,\\ldots,\\beta_k\\right) = |\\beta_1| + \\ldots + |\\beta_k| $, a function that penalizes complexity in $\\beta$\\\\\n",
    "\t  \n",
    "Minimizing $\\beta$-values, will give sparse model (meaning: many $\\beta$-values exactly equal to $0$) \n",
    "\n",
    "To modulate the degree of sparsity, we change $\\lambda$.\n",
    "\n",
    "Adding penalties for  complexity  is a general strategy. One could consider choosing $\\beta_0,\\beta_1,\\ldots, \\beta_k$, to minimize\n",
    "\t\\[\n",
    "\t\\operatorname{avg}L\\left( {\\rm outcome} ,  {\\beta_0 + \\beta_1 x_1 + ... \\beta_k x_k}\\right) + \\lambda P\\left(\\beta_1,\\ldots,\\beta_k\\right)\n",
    "\t\\]\n",
    " with a variety of different $P$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Another popular choice is $P(\\beta_1,\\ldots,\\beta_k) = \\beta_1^2 + \\ldots + \\beta_k^2$.\n",
    " \n",
    " **Ridge Regression** or **Tikhonov Regularization**   Does not result in a *sparse model*. In all these cases increasing $\\lambda$ results in a *less complex* predictive model.\n",
    " \n",
    "\n",
    "In some cases we may: \t\n",
    "- First grow our feature set by creating meta-features\n",
    "- Then select the useful meta-features by using a \\textbf{lasso} fit\n",
    "\n",
    "\n",
    "For high dimensional problems, expand each variable\n",
    "\t\\vspace{3mm}\n",
    "\t\\begin{align*}\n",
    "\t&\\,\\,\\,\\,\\,\\,\\,\\,\\begin{pmatrix}\n",
    "\t& & & \\\\  x_1 , &  x_2 , &  \\cdots &  x_k \\\\ & & &\n",
    "\t\\end{pmatrix}\\\\\n",
    "\t&\\rightarrow\n",
    "\t\\begin{pmatrix}\n",
    "\t& & & & & & & & & \\\\ \n",
    "\t x_1 , &  \\cdots , &  x_1^m , & \n",
    "\t x_2 , &  \\cdots , &  x_2^m , &\n",
    "\t\\cdots, &\n",
    "\t x_k , &  \\cdots , &  x_k^m \\\\\n",
    "\t& & & & & & & & &\n",
    "\t\\end{pmatrix}\n",
    "\t\\end{align*}\n",
    "    \n",
    "\tand use the **lasso** on this expanded problem.\n",
    "\t- $m$ should be small ($\\sim 5$ish) \n",
    "\t- **Spline** basis generally outperforms **polynomial**\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "    More General Machine Learning Methods\n",
    "\tThere are many other methods that can be used for prediction:\n",
    "\n",
    "Just to name some examples:\n",
    "- Regression Trees (boosted trees, random forests)\n",
    "- Neural Networks (deep learning)\n",
    "- Support Vector Machines\n",
    "\n",
    "All of these work in a very similar way, that is actually not that complicated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Validation \n",
    "\n",
    "Split Sample Validation - Lasso\n",
    "\n",
    "For choosing $\\lambda$ in the lasso...\n",
    "- Split your data into a <span style=\"color:blue\"> training</span> set and a <span style=\"color:orange\"> validation</span> set\n",
    "- Choose candidate $\\lambda$-values ($\\lambda_1,\\ldots,\\lambda_m$)\n",
    "- For each candidate $\\lambda$-value, fit your Lasso model on the <span style=\"color:blue\"> training</span> data to get a set of coefficients\\\\\n",
    "- Evaluate each of the $m$ sets of coefficients on your <span style=\"color:orange\"> validation</span> data.\n",
    "\n",
    "<span style=\"color:red\"> Final Step (why?):</span> Refit the Lasso model to **all the data**, using the best $\\lambda$-value. \n",
    "\n",
    "\n",
    "More Generally\n",
    "\n",
    "Suppose we are deciding between a <span style=\"color:blue\"> neural network </span>, a <span style=\"color:red\"> regression tree with depth $3$</span>, and a <span style=\"color:orange\"> regression tree with depth $5$</span>.\\\\\n",
    "- Split your data into a training set and a validation set\n",
    "- For each of our $3$ procedures, build a predictive model on the training data\n",
    "- Evaluate each of the $3$ predictive models on the validation data.\n",
    "\n",
    "<span style=\"color:red\"> Final Step:</span> Refit a predictive model to the full data, using the **best** procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
