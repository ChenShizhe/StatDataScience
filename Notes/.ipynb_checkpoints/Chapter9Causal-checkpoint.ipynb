{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 Causal Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Two Paradoxes \n",
    "\n",
    "\n",
    "**Simpson's paradox.** The following table summarizes number of successes and fails for two treatments (A and B) for small and large kidney stones. Treatment A is an open surgical procedures, and Treatment B is a minimally-invasive procedure. A more successful treatment should yield higher success rate compare to its alternative. \n",
    "\n",
    "|     | Small | stones |  Large | stones |\n",
    "| :- | :-:| :-: | :-: | :-: |\n",
    "|  | success | fail | success | fail   |\n",
    "| Treatment A | 81 | 6 | 192 |71   |\n",
    "| Treatment B | 234 | 36 | 55 |25   |\n",
    "\n",
    "We can calculate the success rates as follow ï¼ˆTreatment A v.s. Treatment B). \n",
    "\n",
    "- Success rate for small stones:   0.93 (81/87)   > 0.87 (234/270)\n",
    "\n",
    "- Success rate for large stones:   0.73  (192/263) > 0.69 (55/80)\n",
    "\n",
    "- Overall success rate:   0.78 (273/350)  <span style=\"color:red\"> <  </span> 0.83 (289/350)\n",
    "\n",
    "\n",
    "<span style = \"color:red\"> Which treatment is more effective? </span> Read more about Simpson's paradox [here](https://en.wikipedia.org/wiki/Simpson's_paradox).\n",
    "\n",
    "\n",
    "**Lord's paradox.** Lord (1967) discussed a paradox concerning whether the  effects of the diet provided in the dining hall differ for males and females. \n",
    "\n",
    "The data is shown in the following scatter plot. The variables involved are gender $(G_i)$, weight in 1963 $(X_i)$, and weight in 1964 $(Y_i)$. We have $\\text{mean}(Y_i \\mid G_i=1)=\\text{mean}(X_i \\mid G_i=1)=150$ and $\\text{mean}(Y_i \\mid G_i=0)=\\text{mean}(X_i \\mid G_i=0)=130$.\n",
    " \n",
    " \n",
    " <img src=\"../Figures/Ch9/lord.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "- Researcher A: Average weights unchanged for both males and females. \n",
    "- Researcher B: $Y_i =\\beta_0+\\beta_g G_i+\\beta_X X_i+\\epsilon_i$ $\\rightsquigarrow$   $\\widehat{\\beta}_g=6.34$.\n",
    "\n",
    "- Statistician C: $Y_i =\\beta_0+\\beta_g G_i+\\beta_X X_i+\\beta G_i X_i+\\epsilon_i$ $\\rightsquigarrow$   $\\widehat{\\beta}=0.036 \\ (\\text{s.e.} = 0.019)$\n",
    "\n",
    "\n",
    "<span style = \"color:red\"> **Who is correct?** </span> Read more about the Lord's paradox [here](https://en.wikipedia.org/wiki/Lord's_paradox). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Overview of causal inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The two paradoxes should, at the very least, make it clear that association does not necessarily imply causation. In our everyday life, one usually never makes an effort to distinguish association from causation---unless this one is an statistician. Furthermore, most of the quantitative methods can only make claims on associations but not causations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However, it is often the causal inference that matters in the real world. For instance, we might be interested in the following questions.\n",
    "- What would happen to the patient if they received treatment $A$ instead of $ B$?\n",
    "- What would happen to the unemployment rate if the U.S. government increased minimum wages?\n",
    "- What would happen to the case number if a state took a different action in April?\n",
    "\n",
    "\n",
    "In all these what-ifs, we notice that it is always comparing _outcomes_ under _different_ conditions for the _same_ subject(s). In other words, **causal inference** is the comparison between _potential outcomes_ under _treatment_ and _control_ for the _same_ unit(s). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "It has been well-known and a common practice that randomized experiments are warranted for any decision and action that involves causal inference. Two prominent examples are the randomized clinical trials for the evaluation of drugs and treatments, and the A/B testing for evaluation of business strategies. \n",
    "\n",
    "Randomization seems to have answered the what-ifs. With a randomized clinical trial, we can compare the outcomes when patients randomly received the two treatments. However, there are some small but crucial caveats. For instance, in a randomized clinical trial, there can not be comparison on the same unit: a patient can not take one treatment and then roll back time to take another treatment. In other words, the comparison is not done on the _same_ unit. After all, is it even possible to strictly limit causal inference only when comparison is conducted on the same unit? We need a \"language\" to rigorously discuss this question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will use the [potential outcome framework](https://en.wikipedia.org/wiki/Rubin_causal_model) in this note. There are many other approaches including the [direct acyclic graphs](https://en.wikipedia.org/wiki/Causal_graph).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose we **observe** a data set with treatment $Z_i$ ($A$ or $B$) and outcome $Y_i$ (1 for success and 0 for fail) for $i=1,2,\\ldots, n$. Further consider the **potential outcomes** $Y_i( \\text{treatment} = A)$ that represents the outcome if subject $i$ receives treatment $A$, and $Y_i( \\text{treatment} = B)$,  the outcome if subject $i$ receives treatment $B$. Using the notation of potential outcomes, we can write the observed outcome as $Y_i(Z_i)$, which shows that only one potential outcome is observed for each unit. The following table show the potential outcomes and the **observed values** in boldface. \n",
    "\n",
    "| Unit $i$ | $Z_i$ | &nbsp; $Y_i(A)$ | &nbsp; $Y_i(B)$ |\n",
    "| ---  | ---  | ---- | ---- |\n",
    "|1  |  A     |   **0**        |      1|\n",
    "|2  |      B    |    0        |       **1** |\n",
    "|3  |       A  |     **0**        |      0 |\n",
    "|4  |       A |      **1**       |      1 |\n",
    "\n",
    "\n",
    "In a real data, we only get to observe the following. \n",
    "\n",
    "\n",
    "| Unit $i$ | $Z_i$ | $Y_i$ | \n",
    "| --- | --- | ---  |\n",
    "|1  |       A     |         **0**        |     \n",
    "|2  |      B    |       **1** |\n",
    "|3  |       A  |          **0**        |     \n",
    "|4  |       A |          **1**       |     \n",
    "|5  |       B  |           **0** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With these new notations, we can see that the what-ifs describe the comparison of potential outcomes, e.g., $Y_i(A)-Y_i(B)$. To be specific, a causal effect is  defined to be the comparison of the  potential outcomes on the **same units**.\n",
    "- **Individual causal effect**:  $Y_i(A)-Y_i(B)$.\n",
    "- **Average causal effect** (ACE): $\\text{mean}\\{Y_i(A)-Y_i(B)\\}$\n",
    "\n",
    "It is important to notice that $\\text{mean}\\{Y_i(A)\\}$ does not equal to $\\text{mean}\\{Y_i \\mid Z_i=A\\} $ in general. We can see this using the examples in the above tables.\n",
    "- $\\text{mean}\\{Y_i(A)\\}$: average of $Y_i(A)$ for units 1, 2, 3, 4.\n",
    "- $\\text{mean}\\{Y_i\\mid Z_i=A\\}$: average of $Y_i$ for units 1,3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The above tables also reveal the fundamental problem of causal inference. That is only one potential outcome is observed.  In randomized experiments, randomization ensures $\\text{mean}\\{Y_i(A)\\} = \\text{mean}\\{Y_i \\mid Z_i=A\\} $, which allows us to estimate the average causal effects. We should always think about what can and what cannot be learned from the observed data.\n",
    "\n",
    "> If your experiment needs statistics, you ought to have done a better experiment.\n",
    ">\n",
    ">        --  Ernest Rutherford \n",
    "        \n",
    "\n",
    "> Regression models often seem to be used to compensate for problems in measurement, data collection, and study design. By the time models are deployed, the scientific position is nearly hopeless. \n",
    "> \n",
    ">        --  David Freedman \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new task that will be often discussed in this chapter is **Identification**: what can be identified if there are infinite amount of data. This tasks concerns the design of experiements or how the data is collected. In contrast, the **Inference** problem, where we study what can be learned given a finite sample, is what we have focused on in most of the statistical methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Potential outcome \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Recall our notation for potential outcome in Section 9.2. Suppose that we observe a data set with treatment $Z_i \\in \\{0,1\\}$ and outcome $Y_i \\in \\{0,1\\}$ for  $i=1,2,\\ldots, n$. Further consider the pair of **potential outcomes** $\\{Y_i(1), Y_i(0)\\}$ for each unit. We only get to observe $Y_i \\equiv Y_i(Z_i)$ in the data set. This notation system can be generalized to case when the treatments take more than two values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Any causal quanity is a function of potential outcomes. We can define the causal effect for unit $i$ as $\\tau_i \\equiv Y_i(1)-Y_i(0)$. We can define more causal effects as long as they are intepretable and meaningful, such as $\\log Y_i(1)-\\log Y_i(0)$, $Y_i(1)/Y_i(0)$, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Some notes on the potential outcomes. \n",
    "1. Potential outcomes are thought to be fixed for each unit. \n",
    "2. The **observed** outcome is random for each unit, because the treatment is random.\n",
    "3. **Potential** outcomes are seen as fixed _attributes_ of each subject. \n",
    "4. Potential outcomes have a distribution across units. \n",
    "5. Treatments determine which potential outcomes to see. \n",
    "6. In real life, only one potential outcome is observed for each unit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is important to note that the notation itselfs introduces assumptions. \n",
    "1. Causal ordering: $Y_i$ cannot causally affect $Z_i$. \n",
    "2. No interference between units.\n",
    "3. Same version of treatments. \n",
    "\n",
    "These are combined in to the *Stable Unit Treatment Value Assumption* (SUTVA). It requires that if $Z_i =Z_i'$, then $Y_i(\\vec{Z})=Y_i(\\vec{Z}')$, where $\\vec{Z}\\equiv (Z_1,Z_2, \\ldots, Z_n)^T$ and $\\vec{Z}'\\equiv (Z'_1,Z'_2, \\ldots, Z'_n)^T$. The SUTVA is violated when there exist simultaneity (feedback efforts), spillover effects, or multiple versions of treatments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In addition, the treatment can not be an **immutable** characteristic, such as sex, race, age, etc. A general rule for causal inference is that there is no causation without manipulation. There are, however, novel strategies to learn/define causal effects for immutable characteristics.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now examine why causal interpretation is easier from randomized experiments. Recall that the average treatment effect is defined as  \n",
    "$$\n",
    "{\\rm ACE} \\equiv \\mathbb{E}[Y(1)-Y(0)]=\\mathbb{E}[Y(1)]-\\mathbb{E}[Y(0)].\n",
    "$$\n",
    "**Randomization of treatments means $\\{Z_i\\}_{i=1}^n \\perp \\{ Y_i(0), Y_i(1)\\}_{i=1}^n$.**\n",
    "\n",
    "We can derive \n",
    "\\begin{align}\n",
    "{\\rm ACE} & = \\mathbb{E}[Y(1)]-\\mathbb{E}[Y(0)]=\\mathbb{E}[Y(1)|Z=1]-\\mathbb{E}[Y(0)|Z=0]\\\\\n",
    "& = \\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0],\n",
    "\\end{align}\n",
    "where the second equality follows from independence between treatments and potential outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 Observational studies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.1 Overview \n",
    "\n",
    "Observational studies are studies when randomization is infeasible. In the potential outcome framework, observational data means that there is no randomization of treatment assignments, i.e., $\\{Y_i(1), Y_i(0)\\}$ is not independent of $Z_i$. \n",
    "\n",
    "Most of real world data are observational. There are a wide range of challenges in analyzing data in each field. Three main statistical challenges are as follows. \n",
    "\n",
    "* Treatment assignment mechanism is often unknown.\n",
    "* There exist observed and unobserved confounders.\n",
    "* Missing data are widely present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.2 Selection bias \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have discussed in Section 9.2 that, in an observational study, the expected difference between the treatment and control group does not equal to the average causal effects (ACE). We can take a closer look at the difference (i.e., bias)\n",
    "$$\n",
    "\\mathbb{E}[Y_i|Z_i =1] - \\mathbb{E}[Y_i | Z_i=0] - {\\rm ACE}={\\rm pr}(Z_i=0) \\big\\{\\mathbb{E}[Y_i(1)|Z_i=1]-\\mathbb{E}[Y_i(1)|Z_i=0] \\big\\}+{\\rm pr}(Z_i=1) \\big\\{\\mathbb{E}[Y_i(0)|Z_i=1]-\\mathbb{E}[Y_i(0)|Z_i=0] \\big\\}. \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To remove the bias, one common assumption in the literature is known as the **ignorability** assumption. Roughtly, the ignorability assumption imposes that, with covariates $X_i$, units are more homegeneous in the outcome distribitions, i.e.,\n",
    "$$\n",
    "Y_i(1) | Z_i = 1, X_i \\sim Y_i(1)|Z_i=0, X_i\n",
    "$$\n",
    "$$\n",
    "Y_i(0) | Z_i = 1, X_i \\sim Y_i(0)|Z_i=0, X_i\n",
    "$$\n",
    "\n",
    "Formally, the ignorability assumption states that $Z_i \\perp Y_i(Z) | X_i$ for $Z=0,1$. This is also known as uncounfoundedness, exogeneity, selection on observables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In addition to the ignorability assumption, we need a few more assumptions to identify the ACE in an observational study. The assumptions are as follows. \n",
    "\n",
    "- $\\{Z_i, Y_i(1), Y_i(0), X_i\\}_{i=1}^n$ are i.i.d. (stronger than SUTVA)\n",
    "- Ignorability\n",
    "- Overlap: $0<{\\rm pr}(Z_i=1| X_i =x)<1$ for any $x$\n",
    "\n",
    "The overlap assumption is a technical requirement for identification. Without overlap, the sets $\\{Z_i=1, X_i\\}$ or $\\{Z_i=0, X_i\\}$ can be empty, which means that no comparison is possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under these assumptions, we can start to decompose the selection bias. The second term becomes \n",
    "\\begin{align}\n",
    "& \\mathbb{E}[Y_i(0)|Z_i=1]-\\mathbb{E}[Y_i|Z=0]\\\\\n",
    "= & \\int_{S_1 \\setminus S} \\mathbb{E}[Y_i(0) | Z_i =1, X_i=x] d F_{X_i|Z_i=1} (x)  - \\int_{S_0 \\setminus S} \\mathbb{E}[Y_i(0) | Z_i =0, X_i=x] d F_{X_i|Z_i=0} (x) \\\\\n",
    "& + \\int_{S} \\mathbb{E}[Y_i(0)| Z_i=0, X_i=x] d \\{F_{X_i|Z_i=1}(x)-F_{X_i|Z_i=0}(x) \\}\\\\\n",
    "& + \\int_{S} \\big\\{\\mathbb{E}[Y_i(0)| Z_i=1, X_i=x] -\\mathbb{E}[Y_i(0)| Z_i=0, X_i=x] \\big\\} d \\{F_{X_i|Z_i=1}(x)\\},\n",
    "\\end{align}\n",
    "where the first two terms are biases due to non-overlaps, the second-to-last term is the bias due to imbalance of observables, and the last term is the bias due to unobservables. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4.3 Elementary strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stratification**\n",
    "\n",
    "\n",
    "We can identify the ${\\rm ACE}$ via the conditional average causal effect as follows \n",
    "\\begin{align}\n",
    "{\\rm ACE} & = \\mathbb{E}[Y_i(1)-Y_i(0)]= \\mathbb{E}\\big\\{ \\mathbb{E}[Y_i(1)-Y_i(0)|X_i] \\big\\}\\\\\n",
    "& = \\int \\big\\{ \\mathbb{E}[Y_i|Z_i=1,X_i]-\\mathbb{E}[Y_i|Z_i,X_i] \\big\\} d F( X).\n",
    "\\end{align}\n",
    "\n",
    "Suppose that $x_i \\in \\{1,2,\\ldots, K\\}$. Under the ignorability assumption that $Y_{i}(Z) \\perp Z_i \\mid X_i$, which means that $Z_i$ is randominzed in subgroup $\\{i: x_i=k\\}$. \n",
    "We then have \n",
    "$$\n",
    "\\hat{\\rm ACE}=\\sum_{k=1}^K \\frac{n_{[k]}}{n} \\hat{\\rm ACE}_{[k]}.\n",
    "$$\n",
    "It is challenging when we have a larger $K$ (e.g., when there are multiple variables). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome regression**\n",
    "\n",
    "We can use linear outcome models for the potential outcomes. \n",
    "$$\n",
    "\\begin{cases}\n",
    "Y_i(1)=\\alpha_1 + x_i'\\gamma_1 + \\epsilon_i(1) & \\mathbb{E}[\\epsilon_i(1)|X_i]=0\\\\\n",
    "Y_i(0)=\\alpha_0 + x_i'\\gamma_0 + \\epsilon_i(0) & \\mathbb{E}[\\epsilon_i(0)|X_i]=0\n",
    "\\end{cases}\n",
    "$$\n",
    "This yields \n",
    "$$\n",
    "\\begin{cases}\n",
    "\\mathbb{E}[ Y_i(1)| x_i]= \\mathbb{E}[ Y_i| Z_i=1, x_i] =\\alpha_1 + x_i'\\gamma_1 \\\\\n",
    "\\mathbb{E}[ Y_i(0)| x_i]= \\mathbb{E}[ Y_i| Z_i=0, x_i] =\\alpha_0 + x_i'\\gamma_0\n",
    "\\end{cases}\n",
    "$$\n",
    "Then \n",
    "$$\n",
    "{\\rm ACE} = \\mathbb{E}\\big\\{ \\mathbb{E}[Y_i(1)-Y_i(0)|X_i] \\big\\}=(\\alpha_1-\\alpha_0)+(\\gamma_1 -\\gamma_0)' \\mathbb{E}[X_i]\n",
    "$$\n",
    "\n",
    "The above equations lead to $Y_i=Z_i Y_i(1)+(1-Z_i) Y_i(0)$ and $\\epsilon_i=Z_i \\epsilon_i(1)+(1-Z_i) \\epsilon_i(0)$  for the observed outcomes. We then have \n",
    "$$\n",
    "Y_i = \\alpha_0 + (\\alpha_1-\\alpha_0)Z_i +X_i'\\gamma_0+Z_i X_i(\\gamma_1 -\\gamma_0)+\\epsilon_i.\n",
    "$$\n",
    "Note that the same does not hold for logistic regression and other nonlinear regression. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Propensity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.1 Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The key of the potential outcome framework is its separation of the science, i.e. ${\\rm pr}(Y_i(1), Y_i(0) | X_i)$ and the treatment assignment mechanism  \n",
    "$$\n",
    "e(X_i, Y_i(1), Y_i(0)) \\equiv {\\rm pr}(Z_i=1 | X_i, Y_i(1), Y_i(0)). \n",
    "$$\n",
    "Under the ignorability assumption, the treatment assignment mechanism $e(X_i)={\\rm pr}(Z_i=1 | X_i)$. This is known as the **propensity score** by Rosenbaum and Rubin (1983). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Theorem** (Propensity score as a dimension reduction tool) Assume that \n",
    "1. $\\{X_i, Y_i(1), Y_i(0), Z_i\\}_{i=1}^n$ are i.i.d. \n",
    "2. $Z_i \\perp \\{Y_i(1), Y_i(0)\\} | X_i$,\n",
    "3. $\\eta \\leq e(X_i) \\leq 1- \\eta$ for some positive $\\eta$. \n",
    "\n",
    "Then $Z_i \\perp \\{Y_i(1), Y_i(0)\\} | e(X_i)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Proof** Just need to show that.\n",
    "$$\n",
    "{\\rm pr}\\{Z=1 | Y(1), Y(0), e(X) \\} = {\\rm pr}\\{ Z=1 | e(X)\\}.\n",
    "$$\n",
    "\n",
    "The left-hand side satisfies that \n",
    "\\begin{align}\n",
    "{\\rm LHS} \\ & = \\mathbb{E}[Z| Y(1), Y(0), e(X)] \\\\\n",
    "& = \\mathbb{E}[ \\mathbb{E}[Z| Y(1), Y(0), e(X), X] | Y(1), Y(0), e(X)] \\\\\n",
    "& = \\mathbb{E}[e(X)| Y(1), Y(0), e(X)] =e(X),\n",
    "\\end{align}\n",
    "where the third equality holds because \n",
    "\\begin{align}\n",
    "& \\mathbb{E}[Z| Y(1), Y(0), e(X), X] \\\\\n",
    "= & \\mathbb{E}[Z| Y(1), Y(0), X] \\ \\ \\ ({\\rm redundancy \\  of }\\ e(X) \\ \\ {\\rm given} \\ X)\\\\\n",
    "= & \\mathbb{E}[Z|  X] \\ \\ \\  {\\rm (Ignorability)} \\\\\n",
    "= & {\\rm pr}(Z=1|X)=e(X).\n",
    "\\end{align}\n",
    "\n",
    "Furthermore, we have \n",
    "\\begin{align}\n",
    "{\\rm RHS} \\ & = \\mathbb{E}[Z|e(X)] = \\mathbb{E}\\big[ \\mathbb{E}[Z|e(X), X ] \\mid e(X) \\big]\\\\\n",
    "& = \\mathbb{E}[e(X)|e(X)]=e(X),\n",
    "\\end{align}\n",
    "where the third equality holds since \n",
    "$$\n",
    "\\mathbb{E}[Z|e(X), X ]= \\mathbb{E}[Z|X]={\\rm pr}(Z=1|X) =e(X). \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Take-aways** \n",
    "\n",
    "Simple case: $e(X) \\in \\{ e_1, e_2, \\ldots, e_K\\}$. Just do stratified analysis! \n",
    "\n",
    "Problem: (1) $e(X)$ usually takes continuous values; (2) $e(\\cdot)$ is unknown. \n",
    "\n",
    "General case: (1) Estimate $e(X_i)$ with logistic regression; (2) Discretize $\\hat{e}(X_i)$ with cutoffs.\n",
    "\n",
    "Problems: (1) Small $K$ leads to bias; Large $K$ leads to no overlap with strata. So how to choose $K$? \n",
    "(2) $\\hat{e}(X)$ is estimated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.2 Propensity score weighting\n",
    "\n",
    "**Theorem** (IP score weighting) Assume that conditions (1), (2), (3) as before, we have \n",
    "$$\n",
    "\\mathbb{E}[Y_i(1)] = \\mathbb{E}\\left[\\frac{Z_i Y_i}{e(X_i)} \\right],\\ \n",
    "\\mathbb{E}[Y_i(0)] = \\mathbb{E}\\left[\\frac{ (1-Z_i) Y_i}{1-e(X_i)} \\right].\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm ACE} \\ = \\mathbb{E}[Y_i(1)-Y_i(0)]=\\mathbb{E}\\left[ \\frac{Z_i Y_i }{e(X_i)}\\right]-\\mathbb{E}\\left[\\frac{ (1-Z_i) Y_i}{1-e(X_i)} \\right].\n",
    "$$\n",
    "\n",
    "Horvitz-Thompson inverse probability weighting \n",
    "$$\n",
    "\\widehat{\\rm ACE}^{\\rm ipw} =\\frac{1}{n} \\sum_{i=1}^n \\frac{Z_i Y_i}{\\hat{e}(X_i)} - \\frac{1}{n} \\sum_{i=1}^n \\frac{(1-Z_i)Y_i}{1-\\hat{e}(X_i)}.\n",
    "$$\n",
    "Hajek inverse probaility weighting \n",
    "$$\n",
    "\\widehat{\\rm ACE}^{\\rm ipw} =\\frac{  \\sum_{i=1}^n \\frac{Z_i Y_i}{\\hat{e}(X_i)}}{\\sum_{i=1}^n \\frac{Z_i }{\\hat{e}(X_i)}} - \\frac{\\sum_{i=1}^n \\frac{(1-Z_i)Y_i}{1-\\hat{e}(X_i)}}{\\sum_{i=1}^n \\frac{1-Z_i}{1-\\hat{e}(X_i)}}.\n",
    "$$\n",
    "\n",
    "The two estimators have the same limit asymptotically. \n",
    "\n",
    "Problems (1) Unstable (trim propensity score) (2) variance not neat (use bootstrap). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.3 Design using propensity scores\n",
    "\n",
    "The central role of the propensity score in designing observational studies. \n",
    "\n",
    "**Theorem** (Balancing property of propensity score) Assume that conditions 1 and 3 hold. We have $Z \\perp X \\mid e(X)$ and for any $h(x)$, given $\\mathbb{E}|h(X)| < \\infty$, \n",
    "$$\n",
    "\\mathbb{E} \\left[ \\frac{Z h(X)}{e(X)} \\right] =\\mathbb{E} \\left[ \\frac{(1-Z) h(x)}{1-e(X)} \\right]\n",
    "$$\n",
    "\n",
    "Why is this theorem useful? \n",
    "\n",
    "* Design of observational studies without outcome \n",
    "* Crucial to check covariate balance\n",
    "* How to choose $h(X)$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5.4  Doubly robustness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We know two equalities of ${\\rm ACE}$. \n",
    "$$\n",
    "{\\rm (i)} \\ {\\rm ACE} \\ = \\mathbb{E} \\left[\\frac{ZY}{e(X)} \\right] - \\mathbb{E} \\left[\\frac{(1-Z)Y}{1-e(X)} \\right] \\ \\ {\\rm (IPW)}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm (ii)} \\ {\\rm ACE} \\ = \\mathbb{E}[\\mu_1(X)]-\\mathbb{E}[\\mu_0(X)],\n",
    "$$\n",
    "where $\\mu_1(X) = \\mathbb{E}[Y(1)\\mid X]$ and $\\mu_0(X)=\\mathbb{E}[Y(0)\\mid X]$. \n",
    "\n",
    "Both (i) and (ii) hold under assumptions. Thus, any convex combination of (i) and (ii) gives ${\\rm ACE}$. We will discuss one case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider \n",
    "$$\n",
    "\\begin{cases}\n",
    "Y(1) = Y(1) - \\mu_1(X) + \\mu_1(X)\\\\\n",
    "Y(0) = Y(0) - \\mu_0(X) + \\mu_0(X)\n",
    "\\end{cases}\n",
    "$$\n",
    "We can derive \n",
    "\\begin{align}\n",
    "{\\rm ACE} \\ = \\ &  \\mathbb{E}[Y(1) - \\mu_1(X)] -\\mathbb{E}[Y(0) - \\mu_0(X)] \\ {\\rm ACE \\ for \\ residuals}\\\\\n",
    "& \\mathbb{E}[\\mu_1(X)]-\\mathbb{E}[\\mu_0(X)] \\ {\\rm ACE \\ for }\\ X \\\\\n",
    "= & \\mathbb{E}\\left[\\frac{Z [Y-\\mu_1(X)] }{e(X)} \\right] - \\mathbb{E}\\left[\\frac{(1-Z) [Y-\\mu_0(X)] }{1-e(X)} \\right]+\\mathbb{E}[\\mu_1(X)] -\\mathbb{E}[\\mu_0(X)]\\  {\\rm IPW}\\\\\n",
    "= & \\mathbb{E}\\left[\\frac{Z [Y-\\mu_1(X)] }{e(X)} +\\mu_1(X) \\right] - \\mathbb{E}\\left[\\frac{(1-Z) [Y-\\mu_0(X)] }{1-e(X)} +\\mu_0(X)\\right].\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Theorem** (Double robustness property) \n",
    "If either $e(X,\\alpha)=e(X)$ or $\\mu_1(X,\\beta_1)=\\mu_1(x)$, then $\\tilde{\\mu}_{1,{\\rm DR}} =\\mathbb{E}[Y(1)]$;\n",
    "If either $e(X,\\alpha)=e(X)$ or $\\mu_0(X,\\beta_0)=\\mu_0(x)$, then $\\tilde{\\mu}_{0,{\\rm DR}} =\\mathbb{E}[Y(0)]$;\n",
    "If either $e(X,\\alpha)=e(X)$ or $[\\mu_0(X,\\beta_0), \\mu_1(X,\\beta_1) ]=[ \\mu_0(x), \\mu_1(x)]$, then ${\\rm ACE}=\\tilde{\\mu}_{1,{\\rm DR}}-\\tilde{\\mu}_{0,{\\rm DR}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Doubly robust estimator**\n",
    "\n",
    "Step 1: Fit a propensity score model $e(X,\\hat{\\alpha})$.\n",
    "\n",
    "Step 2: Fit two outcome models $\\mu_1(X,\\hat{\\beta}_1)$ and $\\mu_0(X,\\hat{\\beta}_0)$.\n",
    "\n",
    "Step 3: \n",
    "\\begin{align}\n",
    "\\hat{\\mu}_{1,{\\rm DR}} & =\\frac{1}{n} \\sum_{i=1}^n \\left\\{ \\frac{Z_i\\big[Y_i -\\mu_1\\big(X_i, \\hat{\\beta}_1\\big) \\big] }{e\\big( X_i, \\hat{\\alpha}\\big)}+\\mu_1\\big(X_i, \\hat{\\beta}_1\\big) \\right\\}\\\\\n",
    "\\hat{\\mu}_{0,{\\rm DR}} & =\\frac{1}{n} \\sum_{i=1}^n \\left\\{ \\frac{(1-Z_i)\\big[Y_i -\\mu_0\\big(X_i, \\hat{\\beta}_0\\big) \\big] }{e\\big( X_i, \\hat{\\alpha}\\big)}+\\mu_0\\big(X_i, \\hat{\\beta}_0\\big) \\right\\}\\\\\n",
    "\\widehat{\\rm ACE}_{\\rm DR} & =\n",
    "\\hat{\\mu}_{1,{\\rm DR}}- \\hat{\\mu}_{0,{\\rm DR}} = \\frac{1}{n}\\sum_{i=1}^n \\hat{\\tau}_{i, {\\rm DR}},\n",
    "\\end{align}\n",
    "where \n",
    "$$\n",
    "\\hat{\\tau}_{i, {\\rm DR}}=\\frac{Z_i\\big[Y_i -\\mu_1\\big(X_i, \\hat{\\beta}_1\\big) \\big] }{e\\big( X_i, \\hat{\\alpha}\\big)}-\\mu_1\\big(X_i, \\hat{\\beta}_1\\big) - \\frac{(1-Z_i)\\big[Y_i -\\mu_0\\big(X_i, \\hat{\\beta}_0\\big) \\big] }{e\\big( X_i, \\hat{\\alpha}\\big)}-\\mu_0\\big(X_i, \\hat{\\beta}_0\\big).\n",
    "$$\n",
    "\n",
    "Variance estimator \n",
    "\n",
    "(i) Lunceford and Davidian (2004, Stat Med)\n",
    "$$\n",
    "\\hat{\\rm var}\\big(\\hat{\\rm ACE}_{\\rm DR} \\big)=\\frac{1}{n(n-1)} \\sum_{i=1}^n \\big(\\hat{\\tau}_{i,{\\rm DR}} - \\hat{\\rm ACE}_{\\rm DR} \\big)^2\n",
    "$$\n",
    "\n",
    "(ii) Bootstrap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 Instrumental variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work-in-progress\n",
    "\n",
    "- Definition of instrumental variable\n",
    "- IV in DAGs\n",
    "- Assumptions of IV\n",
    "- Two-stage least squares estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
